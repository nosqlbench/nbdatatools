/*
 * Copyright (c) nosqlbench
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

package io.nosqlbench.vshapes.extract;

import io.nosqlbench.vshapes.extract.ModeDetector.ModeDetectionResult;
import io.nosqlbench.vshapes.model.*;
import io.nosqlbench.vshapes.stream.StreamingHistogram;
import org.apache.commons.math3.distribution.BetaDistribution;
import org.apache.commons.math3.distribution.NormalDistribution;
import org.junit.jupiter.api.Test;

import java.util.*;

import static org.assertj.core.api.Assertions.*;

/**
 * Diagnostic test to investigate why 3-mode composite regeneration
 * fails round-trip mode detection.
 *
 * This replicates dim 15 from the testscript_roundtrip_small test:
 * - Gen: ‚äï3[ùí©,Œ≤,‚ñ≠] - 32%Normal(-0.5,œÉ=0.06), 30%Beta(4.28,2.58)[-0.15,0.15], 39%Uniform[0.43,0.58]
 * - Ext: ‚äï3[‚ñ≠,ùí©,Œ≤] - same structure, slightly different order
 * - R-T: Œ≤(0.84,0.56) - FAILED - detected as single Beta instead of 3-mode
 */
class CompositeRegenerationDiagnosticTest {

    @Test
    void diagnoseThreeModeCompositeRegeneration() {
        // Create the 3-mode composite model (matching dim 15 Ext model)
        // Modes at: -0.5 (Normal), ~0 (Beta), ~0.5 (Uniform)
        double normalMean = -0.5, normalSigma = 0.06;
        double betaAlpha = 4.24, betaBeta = 2.55, betaMin = -0.15, betaMax = 0.15;
        double uniformMin = 0.43, uniformMax = 0.57;

        double[] weights = {0.31, 0.30, 0.39};  // Normal, Beta, Uniform

        NormalScalarModel normalComponent = new NormalScalarModel(normalMean, normalSigma);
        BetaScalarModel betaComponent = new BetaScalarModel(betaAlpha, betaBeta, betaMin, betaMax);
        UniformScalarModel uniformComponent = new UniformScalarModel(uniformMin, uniformMax);

        CompositeScalarModel composite = new CompositeScalarModel(
            List.of(normalComponent, betaComponent, uniformComponent),
            weights
        );

        System.out.println("=== 3-Mode Composite Regeneration Diagnostic ===");
        System.out.println();
        System.out.println("Source composite model:");
        System.out.printf("  - Normal(%.2f, œÉ=%.2f): %.0f%%%n", normalMean, normalSigma, weights[0] * 100);
        System.out.printf("  - Beta(%.2f, %.2f) on [%.2f, %.2f]: %.0f%%%n",
            betaAlpha, betaBeta, betaMin, betaMax, weights[1] * 100);
        System.out.printf("  - Uniform[%.2f, %.2f]: %.0f%%%n", uniformMin, uniformMax, weights[2] * 100);
        System.out.println();

        // Generate samples from the composite using inverse CDF sampling
        int sampleCount = 100_000;
        float[] samples = new float[sampleCount];
        Random rng = new Random(42);

        // Apache Commons Math distributions for inverse CDF
        NormalDistribution normalDist = new NormalDistribution(normalMean, normalSigma);
        BetaDistribution betaDist = new BetaDistribution(betaAlpha, betaBeta);

        // Build cumulative weights
        double[] cumWeights = new double[weights.length];
        double cumSum = 0;
        for (int i = 0; i < weights.length; i++) {
            cumSum += weights[i];
            cumWeights[i] = cumSum;
        }

        // Sample from each component
        int[] componentCounts = new int[3];
        for (int i = 0; i < sampleCount; i++) {
            double u = rng.nextDouble();

            // Select component
            int comp = 0;
            for (int c = 0; c < cumWeights.length; c++) {
                if (u < cumWeights[c]) {
                    comp = c;
                    break;
                }
            }
            componentCounts[comp]++;

            // Sample from selected component using inverse CDF
            double u2 = rng.nextDouble();
            double value;
            switch (comp) {
                case 0 -> value = normalDist.inverseCumulativeProbability(u2);
                case 1 -> {
                    // Scale Beta[0,1] to [betaMin, betaMax]
                    double betaVal = betaDist.inverseCumulativeProbability(u2);
                    value = betaMin + betaVal * (betaMax - betaMin);
                }
                case 2 -> value = uniformMin + u2 * (uniformMax - uniformMin);
                default -> throw new IllegalStateException();
            }

            samples[i] = (float) value;
        }

        System.out.printf("Generated %,d samples%n", sampleCount);
        System.out.printf("  - Component 0 (Normal): %,d samples (%.1f%%)%n",
            componentCounts[0], 100.0 * componentCounts[0] / sampleCount);
        System.out.printf("  - Component 1 (Beta): %,d samples (%.1f%%)%n",
            componentCounts[1], 100.0 * componentCounts[1] / sampleCount);
        System.out.printf("  - Component 2 (Uniform): %,d samples (%.1f%%)%n",
            componentCounts[2], 100.0 * componentCounts[2] / sampleCount);
        System.out.println();

        // Build histogram to visualize
        int numBins = 50;
        float min = Float.MAX_VALUE, max = Float.MIN_VALUE;
        for (float v : samples) {
            min = Math.min(min, v);
            max = Math.max(max, v);
        }
        float binWidth = (max - min) / numBins;

        int[] histogram = new int[numBins];
        for (float v : samples) {
            int bin = Math.min((int) ((v - min) / binWidth), numBins - 1);
            histogram[bin]++;
        }

        // Print histogram sparkline
        int maxCount = Arrays.stream(histogram).max().orElse(1);
        StringBuilder sparkline = new StringBuilder();
        char[] blocks = {'‚ñÅ', '‚ñÇ', '‚ñÉ', '‚ñÑ', '‚ñÖ', '‚ñÜ', '‚ñá', '‚ñà'};
        for (int count : histogram) {
            int level = (int) (7.0 * count / maxCount);
            level = Math.max(0, Math.min(7, level));
            sparkline.append(blocks[level]);
        }

        System.out.println("Histogram of regenerated data (50 bins):");
        System.out.println("  " + sparkline);
        System.out.println();

        // Print detailed histogram
        System.out.println("Bin details:");
        for (int i = 0; i < numBins; i++) {
            float binStart = min + i * binWidth;
            float binEnd = binStart + binWidth;
            double pct = 100.0 * histogram[i] / sampleCount;
            String bar = "‚ñà".repeat(Math.min((int)(pct * 2), 50));
            System.out.printf("  [%6.3f,%6.3f]: %5d (%.2f%%) %s%n",
                binStart, binEnd, histogram[i], pct, bar);
        }
        System.out.println();

        // Run mode detection
        System.out.println("=== Mode Detection Results ===");

        // Test with different max modes
        for (int maxModes : new int[]{3, 5, 10}) {
            ModeDetectionResult result = ModeDetector.detectAdaptive(samples, maxModes);
            System.out.printf("ModeDetector.detectAdaptive(maxModes=%d):%n", maxModes);
            System.out.printf("  - Mode count: %d%n", result.modeCount());
            System.out.printf("  - Is multimodal: %s%n", result.isMultimodal());
            System.out.printf("  - Dip statistic: %.4f%n", result.dipStatistic());
            if (result.modeCount() > 1) {
                System.out.println("  - Peak locations: " + Arrays.toString(result.peakLocations()));
                System.out.println("  - Mode weights: " + Arrays.toString(result.modeWeights()));
            }
            System.out.println();
        }

        // Try fitting with BestFitSelector
        System.out.println("=== Best Fit Selection ===");

        // Try parametric first
        BestFitSelector parametric = BestFitSelector.boundedDataSelector();
        try {
            ComponentModelFitter.FitResult paramResult = parametric.selectBestResult(samples);
            System.out.printf("Parametric best fit: %s (KS=%.4f)%n",
                paramResult.modelType(), paramResult.goodnessOfFit());
        } catch (Exception e) {
            System.out.println("Parametric fit failed: " + e.getMessage());
        }

        // Try strict round-trip selector
        BestFitSelector roundTrip = BestFitSelector.strictRoundTripSelector(3);
        try {
            ComponentModelFitter.FitResult rtResult = roundTrip.selectBestResult(samples);
            System.out.printf("Round-trip best fit: %s (KS=%.4f)%n",
                rtResult.modelType(), rtResult.goodnessOfFit());

            if (rtResult.model() instanceof CompositeScalarModel comp) {
                System.out.printf("  Components: %d%n", comp.getComponentCount());
                for (int i = 0; i < comp.getComponentCount(); i++) {
                    ScalarModel c = comp.getScalarModels()[i];
                    System.out.printf("  - %.0f%% %s%n",
                        comp.getWeights()[i] * 100, c.getModelType());
                }
            }
        } catch (Exception e) {
            System.out.println("Round-trip fit failed: " + e.getMessage());
        }

        // Assertion: mode detection should find at least 2 modes
        ModeDetectionResult finalResult = ModeDetector.detectAdaptive(samples, 3);
        assertThat(finalResult.isMultimodal())
            .as("3-mode composite regeneration should be detected as multimodal")
            .isTrue();
        assertThat(finalResult.modeCount())
            .as("Should detect at least 2 modes (ideally 3)")
            .isGreaterThanOrEqualTo(2);
    }

    @Test
    void diagnoseNarrowNormalMode() {
        // Test specifically the narrow Normal mode detection
        // The Normal at -0.5 with œÉ=0.06 is very narrow (6% of range)

        System.out.println("=== Narrow Normal Mode Detection ===");

        // Just Normal samples
        NormalDistribution narrowNormalDist = new NormalDistribution(-0.5, 0.06);
        float[] normalSamples = new float[30000];  // ~30% weight
        Random rng = new Random(42);
        for (int i = 0; i < normalSamples.length; i++) {
            normalSamples[i] = (float) narrowNormalDist.inverseCumulativeProbability(rng.nextDouble());
        }

        // Check the spread
        float minN = Float.MAX_VALUE, maxN = Float.MIN_VALUE;
        for (float v : normalSamples) {
            minN = Math.min(minN, v);
            maxN = Math.max(maxN, v);
        }
        System.out.printf("Normal(-0.5, œÉ=0.06) samples: min=%.3f, max=%.3f%n", minN, maxN);
        System.out.printf("Spread: %.3f (%.1f%% of [-1,1] range)%n",
            maxN - minN, (maxN - minN) / 2.0 * 100);

        // Normal distribution has long tails, so with 30k samples some extreme values expected
        // The 99.7% rule says 99.7% within 3œÉ = 0.18, but tails can extend further
        // With 30k samples, expect ~90 samples beyond ¬±3œÉ
        assertThat(maxN - minN)
            .as("Normal with œÉ=0.06 should have most samples within reasonable range")
            .isLessThan(1.0f);  // Relaxed to account for Normal distribution tails
    }

    @Test
    void testStreamingHistogramMultimodalDetection() {
        // This test verifies that StreamingHistogram.isMultiModal() works correctly
        // for 3-mode composite data with the EXACT prominence threshold used in R-T extraction
        // Using ACTUAL Gen dim 15 parameters from testscript_roundtrip_small

        System.out.println("=== StreamingHistogram Multimodal Detection (Actual Gen dim 15 params) ===");

        // ACTUAL Gen dim 15 parameters from test output:
        // 28% Normal(-0.5, œÉ=0.07), 35% Beta(2.79, 4.33)[-0.15, 0.15], 37% Uniform[0.43, 0.58]
        double normalMean = -0.5, normalSigma = 0.07;
        double betaAlpha = 2.79, betaBeta = 4.33, betaMin = -0.15, betaMax = 0.15;
        double uniformMin = 0.43, uniformMax = 0.58;
        double[] weights = {0.28, 0.35, 0.37};

        NormalDistribution normalDist = new NormalDistribution(normalMean, normalSigma);
        BetaDistribution betaDist = new BetaDistribution(betaAlpha, betaBeta);

        double[] cumWeights = new double[weights.length];
        double cumSum = 0;
        for (int i = 0; i < weights.length; i++) {
            cumSum += weights[i];
            cumWeights[i] = cumSum;
        }

        int sampleCount = 100_000;
        float[] samples = new float[sampleCount];
        Random rng = new Random(42);

        for (int i = 0; i < sampleCount; i++) {
            double u = rng.nextDouble();
            int comp = 0;
            for (int c = 0; c < cumWeights.length; c++) {
                if (u < cumWeights[c]) {
                    comp = c;
                    break;
                }
            }

            double u2 = rng.nextDouble();
            double value;
            switch (comp) {
                case 0 -> value = normalDist.inverseCumulativeProbability(u2);
                case 1 -> {
                    double betaVal = betaDist.inverseCumulativeProbability(u2);
                    value = betaMin + betaVal * (betaMax - betaMin);
                }
                case 2 -> value = uniformMin + u2 * (uniformMax - uniformMin);
                default -> throw new IllegalStateException();
            }
            samples[i] = (float) value;
        }

        // Create StreamingHistogram with 100 bins (same as StreamingModelExtractor)
        StreamingHistogram histogram = new StreamingHistogram(100);
        for (float v : samples) {
            histogram.add(v);
        }

        // Test with different prominence thresholds
        double[] thresholds = {0.05, 0.10, 0.15, 0.20, 0.25, 0.30};
        System.out.println("\nStreamingHistogram.isMultiModal() with different prominence thresholds:");
        for (double threshold : thresholds) {
            boolean isMultimodal = histogram.isMultiModal(threshold);
            boolean hasGaps = histogram.hasSignificantGaps(threshold);
            List<StreamingHistogram.Mode> modes = histogram.findModes(threshold);

            System.out.printf("  Threshold=%.2f: isMultimodal=%s, hasGaps=%s, modes=%d%n",
                threshold, isMultimodal, hasGaps, modes.size());
            if (!modes.isEmpty()) {
                System.out.printf("    Mode locations: ");
                for (StreamingHistogram.Mode mode : modes) {
                    System.out.printf("%.3f ", mode.value());
                }
                System.out.println();
            }
        }

        // The CRITICAL test: does it detect multimodality with prominence=0.25?
        // This is what extractWithStreamingExtractor uses
        boolean multimodalAt025 = histogram.isMultiModal(0.25);
        boolean hasGapsAt025 = histogram.hasSignificantGaps(0.25);
        List<StreamingHistogram.Mode> modesAt025 = histogram.findModes(0.25);

        System.out.println("\n=== CRITICAL: prominence=0.25 (used in R-T extraction) ===");
        System.out.printf("  isMultiModal(0.25) = %s%n", multimodalAt025);
        System.out.printf("  hasSignificantGaps(0.25) = %s%n", hasGapsAt025);
        System.out.printf("  modes found = %d%n", modesAt025.size());

        // Compare ModeDetector.detect() vs detectAdaptive() - THIS IS CRITICAL
        // StreamingModelExtractor uses detect() (not detectAdaptive()) when useAdaptiveResolution=false
        System.out.println("\n=== ModeDetector comparison (detect vs detectAdaptive) ===");

        ModeDetectionResult detectResult = ModeDetector.detect(samples, 3);
        System.out.println("ModeDetector.detect(maxModes=3):");
        System.out.printf("  modeCount = %d%n", detectResult.modeCount());
        System.out.printf("  isMultimodal = %s%n", detectResult.isMultimodal());
        System.out.printf("  dipStatistic = %.4f%n", detectResult.dipStatistic());

        ModeDetectionResult detectAdaptiveResult = ModeDetector.detectAdaptive(samples, 3);
        System.out.println("\nModeDetector.detectAdaptive(maxModes=3):");
        System.out.printf("  modeCount = %d%n", detectAdaptiveResult.modeCount());
        System.out.printf("  isMultimodal = %s%n", detectAdaptiveResult.isMultimodal());
        System.out.printf("  dipStatistic = %.4f%n", detectAdaptiveResult.dipStatistic());

        // The assertion: StreamingHistogram should detect multimodality with 0.25 threshold
        // OR hasSignificantGaps should return true
        assertThat(multimodalAt025 || hasGapsAt025)
            .as("3-mode composite should be detected as multimodal OR have significant gaps")
            .isTrue();

        // CRITICAL: Both detect() and detectAdaptive() should find multimodality
        // If detect() fails but detectAdaptive() succeeds, that's the bug!
        assertThat(detectResult.isMultimodal())
            .as("ModeDetector.detect() should also find multimodality for 3-mode composite")
            .isTrue();
    }
}
